{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f727f54f",
   "metadata": {},
   "source": [
    "# Data\n",
    "harry potter book - https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books?resource=download  \n",
    "alice - https://www.kaggle.com/datasets/leelatte/alicetxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3adbe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/mnt/d/Data/llm_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d2c81fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def clean_data(filr_dir, file_name):\n",
    "\n",
    "    with open(f\"{filr_dir}/{file_name}\", \"r\", encoding=\"utf-8\") as file:\n",
    "        book_text = file.read()\n",
    "\n",
    "    cleaned_data = re.sub(r\"\\n+\", \" \", book_text)  # 줄 바꿈을 하나의 공백으로 변경\n",
    "    cleaned_data = re.sub(r\"\\s\", \" \", cleaned_data)  # 여러 공백을 하나의 공백으로 변경\n",
    "\n",
    "    print(f\"cleaned_{file_name, len(cleaned_data)} characters\")  # 글자 수 출력\n",
    "\n",
    "    save_dir = filr_dir[:-9]\n",
    "    # save_file_name = file_name.split(\"/\")[-1]\n",
    "\n",
    "    with open(f\"{save_dir}/cleaned/{file_name}\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed791d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_('01 Harry Potter and the Sorcerers Stone.txt', 436001) characters\n",
      "cleaned_('02 Harry Potter and the Chamber of Secrets.txt', 488771) characters\n",
      "cleaned_('03 Harry Potter and the Prisoner of Azkaban.txt', 621137) characters\n",
      "cleaned_('04 Harry Potter and the Goblet of Fire.txt', 1093670) characters\n",
      "cleaned_('05 Harry Potter and the Order of the Phoenix.txt', 1489734) characters\n",
      "cleaned_('06 Harry Potter and the Half-Blood Prince.txt', 982041) characters\n",
      "cleaned_('07 Harry Potter and the Deathly Hallows.txt', 1133063) characters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "harry_potter_dir = \"/mnt/d/Data/llm_data/harry_potter/original\"\n",
    "harry_potter_file = os.listdir(harry_potter_dir)\n",
    "\n",
    "for file in harry_potter_file:\n",
    "    clean_data(harry_potter_dir, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74332cc6",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf65266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "글자 수 : 26, 토큰 수 : 6\n",
      "[18308, 14179, 373, 257, 18731, 13]\n",
      "Harry Potter was a wizard.\n",
      "18308\t -> Harry\n",
      "14179\t ->  Potter\n",
      "373\t ->  was\n",
      "257\t ->  a\n",
      "18731\t ->  wizard\n",
      "13\t -> .\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Harry Potter was a wizard.\"\n",
    "\n",
    "tokens = tokenizer.encode(text)\n",
    "\n",
    "print(f\"글자 수 : {len(text)}, 토큰 수 : {len(tokens)}\")\n",
    "print(tokens)  # 인코딩\n",
    "print(tokenizer.decode(tokens))  # 디코딩\n",
    "\n",
    "for t in tokens:\n",
    "    print(f\"{t}\\t -> {tokenizer.decode([t])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dd52e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H -> [39] -> H\n",
      "a -> [64] -> a\n",
      "r -> [81] -> r\n",
      "r -> [81] -> r\n",
      "y -> [88] -> y\n",
      "  -> [220] ->  \n",
      "P -> [47] -> P\n",
      "o -> [78] -> o\n",
      "t -> [83] -> t\n",
      "t -> [83] -> t\n",
      "e -> [68] -> e\n",
      "r -> [81] -> r\n",
      "  -> [220] ->  \n",
      "w -> [86] -> w\n",
      "a -> [64] -> a\n",
      "s -> [82] -> s\n",
      "  -> [220] ->  \n",
      "a -> [64] -> a\n",
      "  -> [220] ->  \n",
      "w -> [86] -> w\n",
      "i -> [72] -> i\n",
      "z -> [89] -> z\n",
      "a -> [64] -> a\n",
      "r -> [81] -> r\n",
      "d -> [67] -> d\n",
      ". -> [13] -> .\n"
     ]
    }
   ],
   "source": [
    "for char in text:\n",
    "    token_ids = tokenizer.encode(char)  # 한 글자씩 인코딩\n",
    "    decoded = tokenizer.decode(token_ids)  # 한 글자씩 디코딩\n",
    "    print(f\"{char} -> {token_ids} -> {decoded}\")\n",
    "    # 공백, 마침표도 토큰화 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c1157",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f4559fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class LLM_Dataset(Dataset):\n",
    "    def __init__(self, txt, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        print(f\"# of tokens in txt: {len(token_ids)}\")\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795180b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens in txt: 117768\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/d/Data/llm_data/harry_potter/cleaned/01 Harry Potter and the Sorcerers Stone.txt\", \"r\", encoding=\"utf-8-sig\") as file:\n",
    "    txt = file.read()\n",
    "\n",
    "dataset = LLM_Dataset(txt=txt, max_length=32, stride=4)\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "accef9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?” said the small voice. “Are you sure? You could be great, you know, it’s all here in your head,\n",
      "” said the small voice. “Are you sure? You could be great, you know, it’s all here in your head, and\n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "\n",
    "x, y = next(data_iter)\n",
    "\n",
    "print(tokenizer.decode(x[0].tolist()))\n",
    "print(tokenizer.decode(y[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_harry_potter_dir = \"/mnt/d/Data/llm_data/harry_potter/cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcaf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
